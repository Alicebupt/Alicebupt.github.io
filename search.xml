<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>&lt;Open Domain Question Answering Using Early Fusion of Knowledge Bases&gt;阅读笔记</title>
      <link href="/2019/05/14/new-article/"/>
      <url>/2019/05/14/new-article/</url>
      
        <content type="html"><![CDATA[<p><strong>题目：</strong> Open Domain Question Answering Using Early Fusion of Knowledge Bases<br>and Text</p><p><strong>来源：</strong> EMNLP2018</p><p><strong>链接：</strong> <a href="https://link.zhihu.com/?target=https%3A//aclweb.org/anthology/D18-1455" target="_blank" rel="noopener"> https://  aclweb.org/anthology/D1  8-1455</a></p><p><strong>源码：</strong> <a href="https://link.zhihu.com/?target=http%3A//github.com/OceanskySun/GraftNet" target="_blank" rel="noopener"> Github</a></p><p><strong>首发：</strong> 转载请注明出处： <a href="https://zhuanlan.zhihu.com/bupt-pris731" target="_blank" rel="noopener"> 学习ML的皮皮虾 </a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>开放域问答的任务是根据问题给出相应的答案，目前的模型已经可以从一个外部的知识库或者是维基百科非结构化的文本中寻找答案，也有人用一些方法将来自两个信息源的预测结果进行聚合，本文称之为后期融合，而本文关注的重点是早期融合，将与问题相关的KB实体和文本放在一起，然后训练单个模型提取答案。</p><p>来自ACL2017的 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1704.08384" target="_blank" rel="noopener"> Question answering on knowledge bases and text using universal<br>schema and memory networks.</a><br>这篇文章基于Key-Value Memory<br>Networks将KB三元组和文本片段分别编码放入记忆模块中，实现两个信息源的早期融合。但是本文作者认为这种方法忽略了KB中的实体与非结构化的文本之间的关联。</p><h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><ol><li>本文将KB实体和文本放入同一个子图，然后训练单个模型从子图中提取答案 </li><li>本文基于图表示学习的方法，并对其进行了改进以适应QA任务：1）异构更新方法；2）定向传播以解决多跳问题 </li></ol><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><strong>Description</strong></p><p>KB：  $\displaystyle K =(V,\varepsilon,R)$  ，其中  $\displaystyle V$  是实体集，<br>$\displaystyle R$  是关系集，  $\displaystyle \varepsilon$  是三元组  $\displaystyle<br>(s,r,o)$  集合，  $\displaystyle r\in R,s\in V,o\in V$</p><p>文本语料库：  $\displaystyle D= \left\{ d_{1},… ,d_{|D|} \right\}$  ，每一项为一个句子，<br>$\displaystyle d_{i}=(w_{1},…,w_{|d_{i}|})$</p><p>链接关系：  $\displaystyle L$  是  $\displaystyle (v,d_{p})$  的集合，表示从KB中的实体到文本的映射，<br>$\displaystyle L_{d}$  表示句子  $\displaystyle d$  中所有的可链接实体。</p><p>任务：在给定问题  $\displaystyle q=(w_{1},…,w_{|q|})$  的条件下，从  $\displaystyle<br>G=(K,D,L)$  中选择答案  $\displaystyle \left\{ a \right\}_{q}$</p><p>本文假设问题的答案是来自KB或者文本句子的实体。</p><p><strong>Solution</strong></p><ol><li>从  $\displaystyle G$  中提取最可能包含答案的子图  $\displaystyle G_{q}$ </li></ol><p>2. 利用本文提出的模型在已知问题  $\displaystyle q$  的条件下学习子图中的节点表示，并判断每个节点是否属于答案。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf6rumj20go07mgn1.jpg" alt></p><p><strong>Retrieval</strong></p><p>KB检索：从问题中提取实体集  $\displaystyle S_{q}$  ，使用PPR方法找到这些实体周围的Top E个实体以及它们之间的关系放入子图<br>$\displaystyle G_{q}$  中。</p><p>文本检索：使用Wikipedia为语料，进行句子级的检索。首先用DrQA中的加权词袋模型检索5篇与问题最相关的文章，根据问题  $\displaystyle<br>q$  检索最相关的Top D个句子并将他们分别作为一个节点加入子图中。</p><p>将KB和文本中的实体进行链接，生成新的子图  $\displaystyle G_{q}=(V_{q},\varepsilon_{q},R^{+})$<br>，其中  $\displaystyle V_{q}=\left\{ v_{1},…,v_{E} \right\} \cup \left\{<br>d_{1},…,d_{D} \right\}$  ，  $\displaystyle R^{+}=R \cup \left\{ r_{L}<br>\right\}$  ，  $\displaystyle \varepsilon_{q}=\left\{ (s,o,r)\in \varepsilon<br>:s,o\in V_{q},r\in R \right\} \cup \left\{ (v,d_{p},r_{L}):(v,d_{p})\in<br>L_{d},d\in V_{q} \right\}$</p><p><strong>GRAFT-Nets</strong></p><p>这个阶段的任务是判断子图中的每个节点是否属于答案，首先要学习子图中节点的表示，然后对节点是否属于答案进行二分类。</p><p>之前图表示学习的过程是：</p><ol><li>初始化每个节点的表示  $\displaystyle h_{v}^{(0)}$  。 </li><li>对于模型的每一层，更新节点的表示  $\displaystyle h_{v}^{(l)}=\phi (h_{v}^{(l-1)},\sum_{v’\in N_{r}(v)}{h_{v’}^{(l-1)}})$ </li></ol><p>与之前的图表示学习相比的不同之处在于：</p><ol><li>子图中节点是异构的，每个节点可能是KB中的实体，也可能是一句自然语言文本。 </li><li>节点的表示需要根据问题（自然语言  $\displaystyle q$  ）来更新。 </li></ol><p><strong>异构更新：</strong></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzfio08nj20go0760tg.jpg" alt></p><p>对于实体节点，</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf6ku4j20ai04074f.jpg" alt></p><p>$\displaystyle h_{v}^{(l-1)}$  ：上一层该节点的表示</p><p>$\displaystyle h_{q}^{(l-1)}$  ：上一层的问题的表示</p><p>$\displaystyle N_{r}(v)$  ：与当前节点相邻的节点，  $\displaystyle \alpha_{r}^{v’}$<br>attention权重，</p><p>$\displaystyle M(v)$  ：  $\displaystyle \left\{ (d,p) \right\}$  ，与实体<br>$\displaystyle v$  相链接的文本及该实体在文本中的位置，  $\displaystyle H_{d,p}^{(l-1)}$<br>是实体在文本中的表示。</p><p>对于文本节点，一行一行的更新</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf9f9bj20cd02p3yk.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf74rhj209w01mdfq.jpg" alt></p><p>$\displaystyle L(d,p)$  ：所有指向句子  $\displaystyle d$  的  $\displaystyle p$<br>位置的实体的集合</p><p><strong>根据问题</strong> $\displaystyle q$  <strong>更新</strong></p><p>$\displaystyle q$  的表示：初始化  $\displaystyle<br>h_{q}^{(0)}=LSTM(w_{1}^{q},…,w_{|q|}^{q})<em>{|q|}\in R^{n}$  更新<br>$\displaystyle h</em>{q}^{(l)}=FFN(\sum_{v\in S_{q}}{h_{v}^{(l)}})$</p><p>在更新实体节点时，根据问题  $\displaystyle q$  来调整相邻节点对该节点的影响</p><p>1）计算注意力权重  $\displaystyle \alpha_{r}^{v’}$  时，  $\displaystyle<br>\alpha_{r}^{v’}=softmax(x_{r}^{T}h_{q}^{(l-1)})$  ，这使得对于该节点的表示更多的依赖于与问题相关的节点。</p><p>2）在相邻节点的表示上，  $\displaystyle<br>\psi_{r}(h_{v’}^{(l-1)})=pr_{v’}^{(l-1)}FFN(x_{r},h_{v’}^{(l-1)})$</p><p>为了实现从源节点到目标节点的多跳，  $\displaystyle pr_{v’}^{(l)}$  来衡量从来自问题的源节点到<br>$\displaystyle v’$  的路径的总权重，计算方式如下：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf9gz4j20cw04n74g.jpg" alt></p><p>这个计算过程基于PageRank算法的思想。将来自问题的源节点的权重一跳一跳的向周边节点传播。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf6qogj20cv04qjrg.jpg" alt></p><p><strong>答案选择</strong></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzgtja93j20bw01jweg.jpg" alt></p><p>根据子图中每个节点的最终的表示进行二分类来选择出答案。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>Dataset</strong></p><p>WikiMovies-10K：包含从<br>WikiMovies数据集中随机选择的10K训练集以及原始的测试集和验证集，并使用该数据集给出的KB和文本语料，用简单的匹配实现实体的链接，并且选择前50的实体和文本放入子图中，子图中答案的召回率有99.6%。</p><p>WebQuestionsSP：包含4737个基于Freebase的实体的问句，被分为训练集3098、测试集1639。从Freebase中问题节点邻域选取500个实体，并从维基百科中选取前50条句子放入子图中。答案的召回率为94%。</p><p>子图统计数据如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf6w02j20go02yaaj.jpg" alt></p><p>为了模拟KB不完整的情况，还构建了来自上述两个数据集的其他三个数据集，将KB中事实的数量分别降采样至10%、30%、50%。</p><p><strong>Result</strong></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbfb21uj20go08ewge.jpg" alt></p><p>从图中可以看出：</p><p>1. GN模型的性能超过了所有的KV模型</p><p>2. GN模型早期融合的效果要比后期聚合更好，而通过融合这两个模型可以得到所有模型中最好的效果。</p><p>3. 当KB变得越来越完备的时候加入文本所带来的提升也在降低。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf6vi3j20am0b30uj.jpg" alt></p><p>上表展示了本文的模型与仅用KB或仅用文本的最好的模型之间的比较，本文的模型几乎都达到了最好的结果。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文着重关注于使用文本和不完整的KB的QA任务，通过改造现有的数据集介绍了这个任务的几个基本问题，并且验证了早期融合比后期聚合效果更好。本文还提出了一个早期融合的模型，GRAFT-<br>Net，来对包含KB实体和文本的子图中的节点进行分类。模型建立在图表示学习的基础上并针对该任务做了相应的修改。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KBQA </tag>
            
            <tag> QA </tag>
            
            <tag> early fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/05/14/hello-world/"/>
      <url>/2019/05/14/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>《Discribing a Knowledge Base》阅读笔记</title>
      <link href="/2018/11/19/discribing-a-knowledge-base-yue-du-bi-ji/"/>
      <url>/2018/11/19/discribing-a-knowledge-base-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>来源：INLG 2018</p><p>论文链接：<a href="https://arxiv.org/pdf/1809.01797.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1809.01797.pdf</a></p><p>源码链接：<a href="https://github.com/EagleW/Describing_a_Knowledge_Base" target="_blank" rel="noopener">https://github.com/EagleW/Describing_a_Knowledge_Base</a></p><p>首发：转载请注明出处：学习ML的皮皮虾</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文的目标是自动化生成关于结构化知识库的自然语言描述，也就是自然语言生成任务。本文的输入 KB 格式如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31sypznjoj20ao08et9q.jpg" alt></p><p>输出的自然语言描述如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31szgyl1yj20gg0b441z.jpg" alt></p><h2 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h2><p>结构化KB的生成与创造性写作有本质上的区别，结构化知识的生成需要覆盖输入中的所有元素，并且要准确、连贯的描述这些元素之间的关联。</p><p>基于语言模型的方法并不能将slot和对应的value对齐,所以经常会将值赋给错误的slot。它也会重复一些相同的slot值.<br>结构化KB的生成中很难捕捉不同的slot之间的依赖性。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>本文的模型以Encoder-Decoder模型为基础，以双向GRU做Encoder，以指针网络和两个attention机制构建了生成框架,该指针网络可以从输入KB中复制事实，两个注意力机制分别为：（i）slot-aware attention: 以捕获slot类型与其对应的value之间的关联; （ii）table-position self-attention: 以捕获相关slot之间的相互依赖性。</p><p>模型图如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t0qyfkzj20f20dgjtc.jpg" alt></p><h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><p>本文将输入的结构化KB看作是一个三元组的列表，L = [(s_{1}, v_{1}, (r_{1}, \hat{r}<em>{1} )), …, (s</em>{n}, v_{n}, (r_{n}, \hat{r}<em>{n} ))] ，其中， s</em>{i} 是一个slot类型， v_{i} 是对应的value， (r_{n}, \hat{r}_{n} ) 表示该三元组在输入中的位置，包括前向和后向。模型的输出是一个段落 Y 。</p><h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><p>本文在L上应用一个双向GRU encoder来生成隐藏状态 H = [h_{1}, h_{2}, …, h_{n}] ，其中 h_{i} 是 I_{i} 的一个隐层状态。 I_{i} = [s_{i}, v_{i}, r_{i}, \hat{r}_{i}] 。</p><h4 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h4><p>Decoder是一个前向的GRU网络，其中隐层状态初始化为 h_{n} 。为了捕捉slot类型和它的value值之间的联系，在decoder端加入了slot-aware attention。在每个时间步长 t ，计算在输入序列上的注意力分布。对于每个输入 i ，给它赋一个注意力权重：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t1xksxrj208201pt8l.jpg" alt></p><p>其中，其中 \tilde{h}^{t} 是第 t 步的隐藏状态。 s_{i} 和 v_{i} 分别是输入的 si和 vi 的嵌入表示。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t36qmfdj202i00mgld.jpg" alt></p><p>表示之前所有步的 \alpha_{t} 的和，表示 t 步之前所有的输入受到的总的关注度。将它作为单独的一项放入attention机制中，避免了重复关注同一位置，从而减少了产生重复文本。 \alpha_{t} 可以被认为是每个输入对生成第 t 个目标字的贡献。</p><p>已知 \alpha_{t} 可以计算：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t4msi1dj204d01zjr7.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t4msicqj2072010dfo.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t4msodnj207r012a9w.jpg" alt></p><p>为了捕捉slot之间的关联，受句子级self-attention捕捉连续的tokens之间的关系的启发，本文提出table position self-attention，并将其与slot-aware attention合并。</p><p>在我们的任务中，由于输入中的大多数三元组按时间顺序组织，我们使用行索引 r 和反向行索引 \hat{r} 来表示输入KB中每个三元组的位置信息。对于每对slot s_{i} 和 s_{j} ，我们计算注意分数 f_{ij} 如下：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t5u0dl2j205p03sq2w.jpg" alt></p><p>其中， W_{in} 、W_{out} 、 W_{g} 是可学习的参数。</p><p>更新</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t6w6ogwj203e028jr7.jpg" alt></p><p>其中，</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t73476dj203m021jr6.jpg" alt></p><h4 id="结构生成器"><a href="#结构生成器" class="headerlink" title="结构生成器"></a>结构生成器</h4><p>传统的序列到序列模型仅通过从具有固定大小的词汇表中选择单词来预测目标序列。但是，在我们的任务中，我们将slot的值视为单个信息单元。受指针网络的启发，设计了一个结构感知生成器。</p><p>我们首先获得所有输入slot值的attention分布 P_{source} 。计算结构感知门 p_{gen}\in[0, 1] 来灵活地选择是从固定的词表中选择一个词还是从输入中选择一个slot-value：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t829ewhj207o010dfn.jpg" alt></p><p>其中， y^{t-1} 是对t-1时刻生成的token的向量表示， \sigma 是Sigmoid函数。</p><p>得到：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t98pltmj207q015dfo.jpg" alt></p><p>loss为：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t98ppb3j206n016mwz.jpg" alt></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h4 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h4><p>结合Wikipedia dump 和Wikidata 创建了一个新的数据集，共收集了106216条数据</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31ta6w9osj20g50340sx.jpg" alt></p><p>与之前用来评估生成效果的Wikibio 数据集相比，本文创建的数据集包含多个句子以尽可能的覆盖输入的结构化KB中的多个事实。这使得生成任务更具挑战性和实用性。</p><h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><p>除了标准的 BLEU，METEOR 和 ROUGE 度量，本文还提出了一种基于KB重建的度量方式，即对于每个实体，从生成的段落中重建KB，然后将其与输入KB进行比较计算准确率、召回率和F值。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31tanzfsej20i404tgmr.jpg" alt></p><h4 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h4><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31tc2pikgj20gf0443yw.jpg" alt></p><p>表5体现了本文模型在基础的指标上有了更好的表现。从表6和表7可以看出，加入两个attention机制的模型在基于重建KB的评价指标上也有更好的表现。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31tci1o8ej2088097q3t.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31tcs8c6nj20gj06x75r.jpg" alt></p><p>由图可见，attention成功的捕捉到了slot之间的相互关联。</p><h4 id="仍存在的问题："><a href="#仍存在的问题：" class="headerlink" title="仍存在的问题："></a>仍存在的问题：</h4><p>在人物实体上的召回率更低，是因为对于一些比较少见的slot类型，没有足够的训练数据。</p><p>生成器会输出一些错误的事实，尤其是在时间的表达上。比如：生成器缺乏一些常识，比如它可能会生成“Aleksei Gasilin ( born 1 March 1996 ) is a Russian Association football Forward (association football). He made his professional debut in the Russian Second Division in 1992 for Russia national under-19 football team.”</p><p>本文的方法有时无法检测到人的性别，从而产生不正确的代词。 对于动物实体，人类作家能够详细阐述。 而本文的系统的输出只会覆盖输入KB中的信息。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文第一次提出了一个表格位置attention，并证明它可以有效地捕获事实之间的相互依赖性。 这种新方法在KB重建时实现了2.5％-7.8％的F值增益。本文提出了基于KB重建的度量方式，从输出语句中提取KB并将其与输入KB进行比较，来衡量在生成器的输出中有多少事实是被正确表达了的。另外，本文还创建了一个大型KB数据集,包含对106216个实体的自然语言描述。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLG </tag>
            
            <tag> KB </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
