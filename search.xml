<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>&lt;Open Domain Question Answering Using Early Fusion of Knowledge Bases&gt;阅读笔记</title>
      <link href="/2019/05/14/new-article/"/>
      <url>/2019/05/14/new-article/</url>
      
        <content type="html"><![CDATA[<p><strong>题目：</strong> Open Domain Question Answering Using Early Fusion of Knowledge Bases<br>and Text</p><p><strong>来源：</strong> EMNLP2018</p><p><strong>链接：</strong> <a href="https://link.zhihu.com/?target=https%3A//aclweb.org/anthology/D18-1455" target="_blank" rel="noopener"> https://  aclweb.org/anthology/D1  8-1455</a></p><p><strong>源码：</strong> <a href="https://link.zhihu.com/?target=http%3A//github.com/OceanskySun/GraftNet" target="_blank" rel="noopener"> Github</a></p><p><strong>首发：</strong> 转载请注明出处： <a href="https://zhuanlan.zhihu.com/bupt-pris731" target="_blank" rel="noopener"> 学习ML的皮皮虾 </a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>开放域问答的任务是根据问题给出相应的答案，目前的模型已经可以从一个外部的知识库或者是维基百科非结构化的文本中寻找答案，也有人用一些方法将来自两个信息源的预测结果进行聚合，本文称之为后期融合，而本文关注的重点是早期融合，将与问题相关的KB实体和文本放在一起，然后训练单个模型提取答案。</p><p>来自ACL2017的 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1704.08384" target="_blank" rel="noopener"> Question answering on knowledge bases and text using universal<br>schema and memory networks.</a><br>这篇文章基于Key-Value Memory<br>Networks将KB三元组和文本片段分别编码放入记忆模块中，实现两个信息源的早期融合。但是本文作者认为这种方法忽略了KB中的实体与非结构化的文本之间的关联。</p><h2 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h2><ol><li>本文将KB实体和文本放入同一个子图，然后训练单个模型从子图中提取答案 </li><li>本文基于图表示学习的方法，并对其进行了改进以适应QA任务：1）异构更新方法；2）定向传播以解决多跳问题 </li></ol><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p><strong>Description</strong></p><p>KB：  $\displaystyle K =(V,\varepsilon,R)$  ，其中  $\displaystyle V$  是实体集，<br>$\displaystyle R$  是关系集，  $\displaystyle \varepsilon$  是三元组  $\displaystyle<br>(s,r,o)$  集合，  $\displaystyle r\in R,s\in V,o\in V$</p><p>文本语料库：  $\displaystyle D= \left\{ d_{1},… ,d_{|D|} \right\}$  ，每一项为一个句子，<br>$\displaystyle d_{i}=(w_{1},…,w_{|d_{i}|})$</p><p>链接关系：  $\displaystyle L$  是  $\displaystyle (v,d_{p})$  的集合，表示从KB中的实体到文本的映射，<br>$\displaystyle L_{d}$  表示句子  $\displaystyle d$  中所有的可链接实体。</p><p>任务：在给定问题  $\displaystyle q=(w_{1},…,w_{|q|})$  的条件下，从  $\displaystyle<br>G=(K,D,L)$  中选择答案  $\displaystyle \left\{ a \right\}_{q}$</p><p>本文假设问题的答案是来自KB或者文本句子的实体。</p><p><strong>Solution</strong></p><ol><li>从  $\displaystyle G$  中提取最可能包含答案的子图  $\displaystyle G_{q}$ </li></ol><p>2. 利用本文提出的模型在已知问题  $\displaystyle q$  的条件下学习子图中的节点表示，并判断每个节点是否属于答案。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf6rumj20go07mgn1.jpg" alt></p><p><strong>Retrieval</strong></p><p>KB检索：从问题中提取实体集  $\displaystyle S_{q}$  ，使用PPR方法找到这些实体周围的Top E个实体以及它们之间的关系放入子图<br>$\displaystyle G_{q}$  中。</p><p>文本检索：使用Wikipedia为语料，进行句子级的检索。首先用DrQA中的加权词袋模型检索5篇与问题最相关的文章，根据问题  $\displaystyle<br>q$  检索最相关的Top D个句子并将他们分别作为一个节点加入子图中。</p><p>将KB和文本中的实体进行链接，生成新的子图  $\displaystyle G_{q}=(V_{q},\varepsilon_{q},R^{+})$<br>，其中  $\displaystyle V_{q}=\left\{ v_{1},…,v_{E} \right\} \cup \left\{<br>d_{1},…,d_{D} \right\}$  ，  $\displaystyle R^{+}=R \cup \left\{ r_{L}<br>\right\}$  ，  $\displaystyle \varepsilon_{q}=\left\{ (s,o,r)\in \varepsilon<br>:s,o\in V_{q},r\in R \right\} \cup \left\{ (v,d_{p},r_{L}):(v,d_{p})\in<br>L_{d},d\in V_{q} \right\}$</p><p><strong>GRAFT-Nets</strong></p><p>这个阶段的任务是判断子图中的每个节点是否属于答案，首先要学习子图中节点的表示，然后对节点是否属于答案进行二分类。</p><p>之前图表示学习的过程是：</p><ol><li>初始化每个节点的表示  $\displaystyle h_{v}^{(0)}$  。 </li><li>对于模型的每一层，更新节点的表示  $\displaystyle h_{v}^{(l)}=\phi (h_{v}^{(l-1)},\sum_{v’\in N_{r}(v)}{h_{v’}^{(l-1)}})$ </li></ol><p>与之前的图表示学习相比的不同之处在于：</p><ol><li>子图中节点是异构的，每个节点可能是KB中的实体，也可能是一句自然语言文本。 </li><li>节点的表示需要根据问题（自然语言  $\displaystyle q$  ）来更新。 </li></ol><p><strong>异构更新：</strong></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzfio08nj20go0760tg.jpg" alt></p><p>对于实体节点，</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf6ku4j20ai04074f.jpg" alt></p><p>$\displaystyle h_{v}^{(l-1)}$  ：上一层该节点的表示</p><p>$\displaystyle h_{q}^{(l-1)}$  ：上一层的问题的表示</p><p>$\displaystyle N_{r}(v)$  ：与当前节点相邻的节点，  $\displaystyle \alpha_{r}^{v’}$<br>attention权重，</p><p>$\displaystyle M(v)$  ：  $\displaystyle \left\{ (d,p) \right\}$  ，与实体<br>$\displaystyle v$  相链接的文本及该实体在文本中的位置，  $\displaystyle H_{d,p}^{(l-1)}$<br>是实体在文本中的表示。</p><p>对于文本节点，一行一行的更新</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf9f9bj20cd02p3yk.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf74rhj209w01mdfq.jpg" alt></p><p>$\displaystyle L(d,p)$  ：所有指向句子  $\displaystyle d$  的  $\displaystyle p$<br>位置的实体的集合</p><p><strong>根据问题</strong> $\displaystyle q$  <strong>更新</strong></p><p>$\displaystyle q$  的表示：初始化  $\displaystyle<br>h_{q}^{(0)}=LSTM(w_{1}^{q},…,w_{|q|}^{q})<em>{|q|}\in R^{n}$  更新<br>$\displaystyle h</em>{q}^{(l)}=FFN(\sum_{v\in S_{q}}{h_{v}^{(l)}})$</p><p>在更新实体节点时，根据问题  $\displaystyle q$  来调整相邻节点对该节点的影响</p><p>1）计算注意力权重  $\displaystyle \alpha_{r}^{v’}$  时，  $\displaystyle<br>\alpha_{r}^{v’}=softmax(x_{r}^{T}h_{q}^{(l-1)})$  ，这使得对于该节点的表示更多的依赖于与问题相关的节点。</p><p>2）在相邻节点的表示上，  $\displaystyle<br>\psi_{r}(h_{v’}^{(l-1)})=pr_{v’}^{(l-1)}FFN(x_{r},h_{v’}^{(l-1)})$</p><p>为了实现从源节点到目标节点的多跳，  $\displaystyle pr_{v’}^{(l)}$  来衡量从来自问题的源节点到<br>$\displaystyle v’$  的路径的总权重，计算方式如下：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf9gz4j20cw04n74g.jpg" alt></p><p>这个计算过程基于PageRank算法的思想。将来自问题的源节点的权重一跳一跳的向周边节点传播。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf6qogj20cv04qjrg.jpg" alt></p><p><strong>答案选择</strong></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzgtja93j20bw01jweg.jpg" alt></p><p>根据子图中每个节点的最终的表示进行二分类来选择出答案。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>Dataset</strong></p><p>WikiMovies-10K：包含从<br>WikiMovies数据集中随机选择的10K训练集以及原始的测试集和验证集，并使用该数据集给出的KB和文本语料，用简单的匹配实现实体的链接，并且选择前50的实体和文本放入子图中，子图中答案的召回率有99.6%。</p><p>WebQuestionsSP：包含4737个基于Freebase的实体的问句，被分为训练集3098、测试集1639。从Freebase中问题节点邻域选取500个实体，并从维基百科中选取前50条句子放入子图中。答案的召回率为94%。</p><p>子图统计数据如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf6w02j20go02yaaj.jpg" alt></p><p>为了模拟KB不完整的情况，还构建了来自上述两个数据集的其他三个数据集，将KB中事实的数量分别降采样至10%、30%、50%。</p><p><strong>Result</strong></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbfb21uj20go08ewge.jpg" alt></p><p>从图中可以看出：</p><p>1. GN模型的性能超过了所有的KV模型</p><p>2. GN模型早期融合的效果要比后期聚合更好，而通过融合这两个模型可以得到所有模型中最好的效果。</p><p>3. 当KB变得越来越完备的时候加入文本所带来的提升也在降低。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g2zzbf6vi3j20am0b30uj.jpg" alt></p><p>上表展示了本文的模型与仅用KB或仅用文本的最好的模型之间的比较，本文的模型几乎都达到了最好的结果。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文着重关注于使用文本和不完整的KB的QA任务，通过改造现有的数据集介绍了这个任务的几个基本问题，并且验证了早期融合比后期聚合效果更好。本文还提出了一个早期融合的模型，GRAFT-<br>Net，来对包含KB实体和文本的子图中的节点进行分类。模型建立在图表示学习的基础上并针对该任务做了相应的修改。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KBQA </tag>
            
            <tag> QA </tag>
            
            <tag> early fusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《基于顺序注意力机制网络的端到端应答选择》和《多轮对话中答案选择的迁移学习方法比较》阅读笔记</title>
      <link href="/2019/03/04/ji-yu-shun-xu-zhu-yi-li-ji-zhi-wang-luo-de-duan-dao-duan-ying-da-xuan-ze-he-duo-lun-dui-hua-zhong-da-an-xuan-ze-de-qian-yi-xue-xi-fang-fa-bi-jiao-yue-du-bi-ji/"/>
      <url>/2019/03/04/ji-yu-shun-xu-zhu-yi-li-ji-zhi-wang-luo-de-duan-dao-duan-ying-da-xuan-ze-he-duo-lun-dui-hua-zhong-da-an-xuan-ze-de-qian-yi-xue-xi-fang-fa-bi-jiao-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<p><strong>来源：</strong> AAAI 2019</p><p><strong>论文1链接：</strong> <a href="https://link.zhihu.com/?target=http%3A//workshop.colips.org/dstc7/papers/07.pdf" target="_blank" rel="noopener"> http://  workshop.colips.org/dst  c7/papers/07.pdf</a></p><p><strong>论文2链接：</strong> <a href="https://link.zhihu.com/?target=http%3A//workshop.colips.org/dstc7/papers/17.pdf" target="_blank" rel="noopener"> http://  workshop.colips.org/dst  c7/papers/17.pdf</a></p><p><strong>首发：转载请注明出处：</strong> 学习ML的皮皮虾</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><strong>Introduction</strong></h2><p>这两篇文章分别来自 <a href="https://link.zhihu.com/?target=http%3A//workshop.colips.org/dstc7/" target="_blank" rel="noopener"> DSTC7</a> 评测任务1 <a href="https://link.zhihu.com/?target=http%3A//workshop.colips.org/dstc7/call.html" target="_blank" rel="noopener"><br>Sentence Selection</a><br>中排名第一和第二的团队，阿里巴巴达摩院以及Palo<br>Alto研究中心。该任务重点关注了面向目标的对话系统中的语句分类技术，参赛者需要结合给定的对话片段在给出的一组候选句中选择最符合的一个应答。参赛者不能使用人工提取的特征和基于规则的系统。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879gy1g321x9nqncj20go08ywfw.jpg" alt></p><p>该任务包含两个 <a href="https://link.zhihu.com/?target=https%3A//ibm.github.io/dstc7-noesis/public/datasets.html" target="_blank" rel="noopener"> 数据集</a><br>：Advising数据集（包含学生和他的选课顾问之间的对话）和Ubuntu数据集（顾客和技术支持者之间的对话）。每个数据集包括两个speaker之间的部分对话，以及下一句应答的一组可选项。</p><p>该任务下又分了5个子任务，它们的不同主要在候选应答的数量，候选应答中正确应答句的数量，以及是否允许访问外部数据集。</p><ol><li>从100个候选句（包含1个正确回复）中选择下一句回复。 </li><li>从120000候选句中选择下一句回复。（仅适用于Ubuntu数据） </li><li>从100个候选句（包含1-5个正确回复）中选择下一句回复。（仅适用于Advising数据） </li><li>从100个候选句（包含0个或1个正确回复）中选择下一句回复或者None。 </li><li>从100个候选句中选择正确回复，可以访问外部数据集。 </li></ol><p>两篇文章都将应答选择看做是一个二元的分类任务，给定一个多轮的对话历史和一个候选应答，模型需要做的就是判断该应答是否为正确的应答句。现在多轮对话的应答选择的典型方法有基于序列的和基于层次的两种。</p><h2 id="Model1（达摩院）"><a href="#Model1（达摩院）" class="headerlink" title="Model1（达摩院）"></a><strong>Model1（达摩院）</strong></h2><p>本文主要研究的问题是如何对对话历史和应答之间的语义关系进行建模。本文用到的模型是为自然语言推理任务而设计的 <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1609.06038" target="_blank" rel="noopener"> ESIM(ACL2017)</a><br>模型，这个模型整体结构图如下所示，主要分为3个部分：Input Encoding，Local Matching 和 Matching<br>Composition。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879gy1g321xx94blj20go0bwgmi.jpg" alt></p><p>Input Encoding</p><p>这部分主要是解决如何对对话历史和应答句进行编码。</p><ol><li>将对话历史拼接成一个长序列 </li><li>对于序列中的每个词将多种词向量拼接，然后用一个带RELU的前馈层压缩维度 </li><li>为了表示上下文信息，将对话历史和应答句分别用BiLSTM进行编码获得隐层状态表示 </li></ol><p>Local Matching</p><p>对对话历史和应答之间的局部语义关系的建模是确定应答是否正确的关键。比如，正确的应答句通常涉及对话历史中的一些关键字，这可以通过局部语义建模来获得。本文使用cross-<br>attention机制来将来自对话历史和应答句的tokens对齐，然后计算token级别的语义关系。attention权重计算方式如下：</p><p>$\displaystyle e_{ij}=(c_{i}^{s})^{T}r_{j}^{s}$</p><p>$\displaystyle \alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{n}{exp(e_{ik})}},<br>c_{i}^{d}=\sum_{j=1}^{n}{\alpha_{ij}r_{j}^{s}}$</p><p>$\displaystyle \beta_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{m}{exp(e_{kj})}},<br>r_{j}^{d}=\sum_{j=1}^{m}{\beta_{ij}c_{i}^{s}}$</p><p>通过比较  $\displaystyle  &lt;c_{i}^{s},c_{i}^{d}&gt;$  就可以获得对齐后的token对之间的语义关系，相同的计算也适用于<br>$\displaystyle &lt;r_{j}^{s},r_{j}^{d}&gt;$  。局部语义比较的方法如下：</p><p>$\displaystyle<br>c_{i}^{l}=F([c_{i}^{s};c_{i}^{d};c_{i}^{s}-c_{i}^{d};c_{i}^{s}\odot<br>c_{i}^{d}])$</p><p>$\displaystyle r_{j}^{l}=F([r_{j}^{s};r_{j}^{d};r_{j}^{s}-r_{j}^{d};r_{j}^{s}<br>\odot r_{j}^{d}])$</p><p>Matching Composition</p><p>确定应答句是否为正确的下一句话，需要一个合成层来合成上面得到的局部匹配的结果。这里再次使用BiLSTM来读取局部匹配向量并学习区分关键的局部匹配向量来获得整体的语义关系<br>。</p><p>$\displaystyle c_{i}^{v}=BiLSTM_{2}(c^{l},i)$</p><p>$\displaystyle r_{j}^{v}=BiLSTM_{2}(r^{l},j)$</p><p>$\displaystyle y=MLP([c_{max}^{v};c_{mean}^{v};r_{max^{v};r_{mean}^{v}}])$</p><p>在Ubuntu数据的子任务2中，需要从12000个候选项中选出正确的应答句，所以需要先从中选出TOP100，然后再将这100个候选应答输入到ESIM主模型中进行排序。这里选出TOP100用了基于句子编码的模型，使用多抽头的self-<br>attention池化的BiLSTM来对句子进行编码，并应用MLP来分类。模型如图所示：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879gy1g321ydq752j20d407u3yv.jpg" alt></p><h2 id="Experiments1"><a href="#Experiments1" class="headerlink" title="Experiments1"></a><strong>Experiments1</strong></h2><p><strong>数据集：</strong></p><p>除了比赛官方提供的两个数据集Ubuntu和Advising之外，本文还在两个大型公开数据集上进行了模型的验证，分别是Lowe Ubuntu数据集和E-<br>commerce数据集。</p><p>训练参数的设置参见论文中的Training Details部分。</p><p><strong>实验结果：</strong></p><p>本文的模型在DSTC7的所有子任务上的结果如下表所示：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879gy1g321z58l56j20aw0c7tam.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/8362e879gy1g321zydisxj20go09igmx.jpg" alt></p><p>DSTC7比赛官方提供的排名<br><img src="http://ww1.sinaimg.cn/large/8362e879gy1g3220ditz6j20go06uq4d.jpg" alt></p><p>上表是本文的模型和其它模型在两个大型公开数据集上的效果比较。其中第一组是基于句子编码的方法，通过不同的编码方式对对话历史和应答进行编码。第二组是基于序列的匹配模型，通常使用Attention机制，这些模型比较了对话历史和应答之间的词级别的关系，比第一组效果更好。第三组包含更复杂的基于层次结构的模型，其中DAM是百度发表在ACL2018上的工作，达到了目前最高的水平，ESIM相较于DAM在效果上也实现了显著的提高。</p><h2 id="Model2（Palo-Alto）"><a href="#Model2（Palo-Alto）" class="headerlink" title="Model2（Palo Alto）"></a><strong>Model2（Palo Alto）</strong></h2><p>本文是基于层次结构的模型，同样用到了ESIM，但是不同于上文将对话历史连接成一个长序列，本文提出的解决方案是将对话历史中的每个语句分别与候选句进行匹配，将所有的匹配结果再输入到合成层进行聚合。</p><p>本文认为单句话与应答之间的关系受这句话在对话历史中的位置以及说这句话的人的影响，也就是说离应答句越远的句子影响应该越小，而对话中不同的角色所说的话对应答选择造成的影响也不同。所以在聚合时，加入了这两方面因素的影响。模型结构如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879gy1g3220q899vj20ds094q3g.jpg" alt></p><p>首先将  $\displaystyle C$  里的每一个  $\displaystyle c$  与  $\displaystyle r$<br>进行比较，这里用的是上文所述的句子对建模模型ESIM和ELMo来获得  $\displaystyle c$  与  $\displaystyle r$<br>之间的匹配关系。不同于直接使用ESIM的输出（分类概率），我们去掉最后一层，并且使用前一层的输出向量，将这个向量定义为  $\displaystyle<br>v_{i}$  。</p><p>然后，基于  $\displaystyle s_{i}$  变换向量  $\displaystyle v_{i}$  。 我们通过将<br>$\displaystyle v_{i}$  传递给具有权重  $\displaystyle W_{s_{i}}$<br>的speaker特定残差层来实现这一点，如图2所示。由于大多数信息与speaker是无关的，所以speaker的影响效果应该是输入向量的偏移而不是一个完全的变换。受speaker影响的输出向量表示为<br>$\displaystyle v_{i}^{‘}$  :</p><p>$\displaystyle v_{i}^{‘} = v_{i} + relu\left( W_{s_{i}}v_{i}\right)$</p><p>然后，我们使用学习权重  $\displaystyle \left\{ \alpha_{i} \right\}$<br>计算这些输出向量的加权平均值。这些权重根据它们在上下文中的位置捕获这些话语的重要性，如图2所示。</p><p>$\displaystyle v_{out} =<br>\frac{\sum_{i=1}^{n}{\alpha_{i}v_{i}^{‘}}}{\sum_{i=1}^{n}{\alpha_{i}}}$</p><p>我们将最后一层应用于加权平均值以获得匹配函数  $\displaystyle F$  :</p><p>$\displaystyle F(C,r) = softmax(W_{F}v_{out})$</p><h2 id="Experiments2"><a href="#Experiments2" class="headerlink" title="Experiments2"></a><strong>Experiments2</strong></h2><p>除了ESIM+EMLo之外，本文还训练了GPT和BERT，效果更好。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879gy1g32216fvfmj209k0800tp.jpg" alt></p><p>下图是在Ubuntu数据集上研究使用不同的对话轮次时模型的性能，如图所示，随着向上下文添加更多对话轮次，所有三个模型的性能最初都会提高，但效果会减弱，在6-8个对话轮次时达到峰值。在某些情况下，继续增加对话轮次会降低性能。<br>对于BERT来说，这是最明显的，当使用6轮时时，它实现了Recall @1 0.557，而没有轮次限制则达到0.530，降低了5.1％。<br>这表明更多的上下文并不总是更好，并且将上下文大小视为优化的超参数可能是有益的。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879gy1g3221l5ooej20980dn0u4.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/8362e879gy1g3221woy2dj209k07u74n.jpg" alt></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h2><p>两篇文章给出了在应答选择方向上的两种典型方法，分别从基于序列和基于层次结构两个方向上阐述了在比赛中所用到的模型，两种方法都用到了来自自然语言推理任务的ESIM模型来进行句子对的建模，都用到cross-<br>attention来获取应答与对话历史之间的语义关联。两种方法在DSTC7任务上的排名证明了它们的有效性，同时在文章中还提到了很多工程上的细节，在此并未一一列举。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> QA </tag>
            
            <tag> AS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Discribing a Knowledge Base》阅读笔记</title>
      <link href="/2018/11/19/discribing-a-knowledge-base-yue-du-bi-ji/"/>
      <url>/2018/11/19/discribing-a-knowledge-base-yue-du-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>来源：INLG 2018</p><p>论文链接：<a href="https://arxiv.org/pdf/1809.01797.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1809.01797.pdf</a></p><p>源码链接：<a href="https://github.com/EagleW/Describing_a_Knowledge_Base" target="_blank" rel="noopener">https://github.com/EagleW/Describing_a_Knowledge_Base</a></p><p>首发：转载请注明出处：学习ML的皮皮虾</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文的目标是自动化生成关于结构化知识库的自然语言描述，也就是自然语言生成任务。本文的输入 KB 格式如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31sypznjoj20ao08et9q.jpg" alt></p><p>输出的自然语言描述如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31szgyl1yj20gg0b441z.jpg" alt></p><h2 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h2><p>结构化KB的生成与创造性写作有本质上的区别，结构化知识的生成需要覆盖输入中的所有元素，并且要准确、连贯的描述这些元素之间的关联。</p><p>基于语言模型的方法并不能将slot和对应的value对齐,所以经常会将值赋给错误的slot。它也会重复一些相同的slot值.<br>结构化KB的生成中很难捕捉不同的slot之间的依赖性。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>本文的模型以Encoder-Decoder模型为基础，以双向GRU做Encoder，以指针网络和两个attention机制构建了生成框架,该指针网络可以从输入KB中复制事实，两个注意力机制分别为：（i）slot-aware attention: 以捕获slot类型与其对应的value之间的关联; （ii）table-position self-attention: 以捕获相关slot之间的相互依赖性。</p><p>模型图如下所示：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t0qyfkzj20f20dgjtc.jpg" alt></p><h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><p>本文将输入的结构化KB看作是一个三元组的列表，L = [(s_{1}, v_{1}, (r_{1}, \hat{r}<em>{1} )), …, (s</em>{n}, v_{n}, (r_{n}, \hat{r}<em>{n} ))] ，其中， s</em>{i} 是一个slot类型， v_{i} 是对应的value， (r_{n}, \hat{r}_{n} ) 表示该三元组在输入中的位置，包括前向和后向。模型的输出是一个段落 Y 。</p><h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><p>本文在L上应用一个双向GRU encoder来生成隐藏状态 H = [h_{1}, h_{2}, …, h_{n}] ，其中 h_{i} 是 I_{i} 的一个隐层状态。 I_{i} = [s_{i}, v_{i}, r_{i}, \hat{r}_{i}] 。</p><h4 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h4><p>Decoder是一个前向的GRU网络，其中隐层状态初始化为 h_{n} 。为了捕捉slot类型和它的value值之间的联系，在decoder端加入了slot-aware attention。在每个时间步长 t ，计算在输入序列上的注意力分布。对于每个输入 i ，给它赋一个注意力权重：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t1xksxrj208201pt8l.jpg" alt></p><p>其中，其中 \tilde{h}^{t} 是第 t 步的隐藏状态。 s_{i} 和 v_{i} 分别是输入的 si和 vi 的嵌入表示。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t36qmfdj202i00mgld.jpg" alt></p><p>表示之前所有步的 \alpha_{t} 的和，表示 t 步之前所有的输入受到的总的关注度。将它作为单独的一项放入attention机制中，避免了重复关注同一位置，从而减少了产生重复文本。 \alpha_{t} 可以被认为是每个输入对生成第 t 个目标字的贡献。</p><p>已知 \alpha_{t} 可以计算：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t4msi1dj204d01zjr7.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t4msicqj2072010dfo.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t4msodnj207r012a9w.jpg" alt></p><p>为了捕捉slot之间的关联，受句子级self-attention捕捉连续的tokens之间的关系的启发，本文提出table position self-attention，并将其与slot-aware attention合并。</p><p>在我们的任务中，由于输入中的大多数三元组按时间顺序组织，我们使用行索引 r 和反向行索引 \hat{r} 来表示输入KB中每个三元组的位置信息。对于每对slot s_{i} 和 s_{j} ，我们计算注意分数 f_{ij} 如下：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t5u0dl2j205p03sq2w.jpg" alt></p><p>其中， W_{in} 、W_{out} 、 W_{g} 是可学习的参数。</p><p>更新</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t6w6ogwj203e028jr7.jpg" alt></p><p>其中，</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t73476dj203m021jr6.jpg" alt></p><h4 id="结构生成器"><a href="#结构生成器" class="headerlink" title="结构生成器"></a>结构生成器</h4><p>传统的序列到序列模型仅通过从具有固定大小的词汇表中选择单词来预测目标序列。但是，在我们的任务中，我们将slot的值视为单个信息单元。受指针网络的启发，设计了一个结构感知生成器。</p><p>我们首先获得所有输入slot值的attention分布 P_{source} 。计算结构感知门 p_{gen}\in[0, 1] 来灵活地选择是从固定的词表中选择一个词还是从输入中选择一个slot-value：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t829ewhj207o010dfn.jpg" alt></p><p>其中， y^{t-1} 是对t-1时刻生成的token的向量表示， \sigma 是Sigmoid函数。</p><p>得到：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t98pltmj207q015dfo.jpg" alt></p><p>loss为：</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31t98ppb3j206n016mwz.jpg" alt></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h4 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h4><p>结合Wikipedia dump 和Wikidata 创建了一个新的数据集，共收集了106216条数据</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31ta6w9osj20g50340sx.jpg" alt></p><p>与之前用来评估生成效果的Wikibio 数据集相比，本文创建的数据集包含多个句子以尽可能的覆盖输入的结构化KB中的多个事实。这使得生成任务更具挑战性和实用性。</p><h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><p>除了标准的 BLEU，METEOR 和 ROUGE 度量，本文还提出了一种基于KB重建的度量方式，即对于每个实体，从生成的段落中重建KB，然后将其与输入KB进行比较计算准确率、召回率和F值。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31tanzfsej20i404tgmr.jpg" alt></p><h4 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h4><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31tc2pikgj20gf0443yw.jpg" alt></p><p>表5体现了本文模型在基础的指标上有了更好的表现。从表6和表7可以看出，加入两个attention机制的模型在基于重建KB的评价指标上也有更好的表现。</p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31tci1o8ej2088097q3t.jpg" alt></p><p><img src="http://ww1.sinaimg.cn/large/8362e879ly1g31tcs8c6nj20gj06x75r.jpg" alt></p><p>由图可见，attention成功的捕捉到了slot之间的相互关联。</p><h4 id="仍存在的问题："><a href="#仍存在的问题：" class="headerlink" title="仍存在的问题："></a>仍存在的问题：</h4><p>在人物实体上的召回率更低，是因为对于一些比较少见的slot类型，没有足够的训练数据。</p><p>生成器会输出一些错误的事实，尤其是在时间的表达上。比如：生成器缺乏一些常识，比如它可能会生成“Aleksei Gasilin ( born 1 March 1996 ) is a Russian Association football Forward (association football). He made his professional debut in the Russian Second Division in 1992 for Russia national under-19 football team.”</p><p>本文的方法有时无法检测到人的性别，从而产生不正确的代词。 对于动物实体，人类作家能够详细阐述。 而本文的系统的输出只会覆盖输入KB中的信息。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文第一次提出了一个表格位置attention，并证明它可以有效地捕获事实之间的相互依赖性。 这种新方法在KB重建时实现了2.5％-7.8％的F值增益。本文提出了基于KB重建的度量方式，从输出语句中提取KB并将其与输入KB进行比较，来衡量在生成器的输出中有多少事实是被正确表达了的。另外，本文还创建了一个大型KB数据集,包含对106216个实体的自然语言描述。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLG </tag>
            
            <tag> KB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/11/18/hello-world/"/>
      <url>/2018/11/18/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
